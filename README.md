# ğŸ’° Employee Salary Prediction using Lasso Regression

## ğŸ“Œ Project Overview

This project focuses on predicting **employee monthly salary** using Machine Learning techniques.
The goal is to build a robust regression model while addressing common challenges such as:

* Overfitting
* Multicollinearity
* Model complexity

To achieve this, we implemented and compared:

âœ… Linear Regression (Baseline)
âœ… Ridge Regression (L2 Regularization)
âœ… Lasso Regression (L1 Regularization)

The final model was deployed as an **interactive Streamlit web application**.

---

## ğŸ¯ Problem Statement

Accurate salary prediction is valuable for:

* Compensation planning
* Budget forecasting
* HR analytics
* Fair pay analysis

This project uses employee attributes like experience, education, and job role to predict salary.

---

## ğŸ“Š Dataset Description

The dataset represents structured HR analytics data containing employee-related features:

| Feature           | Description           |
| ----------------- | --------------------- |
| Age               | Employee age          |
| Gender            | Male / Female         |
| Department        | Functional department |
| JobRole           | Employee designation  |
| EducationLevel    | Qualification         |
| YearsExperience   | Total work experience |
| PerformanceRating | Performance score     |
| WorkHoursPerWeek  | Weekly work hours     |
| MonthlySalary     | **Target Variable**   |

---

## ğŸ§  Machine Learning Approach

### **1ï¸âƒ£ Linear Regression**

Linear Regression serves as the **baseline model** for comparison.

The objective is to minimize:

[
Loss = RSS = \sum (y_i - \hat{y}_i)^2
]

Where:

* ( y_i ) â†’ Actual salary
* ( \hat{y}_i ) â†’ Predicted salary
* **RSS** â†’ Residual Sum of Squares

---

### **2ï¸âƒ£ Ridge Regression (L2 Regularization)**

Ridge Regression adds an **L2 penalty** to reduce model complexity and handle multicollinearity.

[
Loss = RSS + Î» \sum w_j^2
]

Where:

* **RSS** â†’ Residual Sum of Squares
* **Î» (lambda)** â†’ Regularization parameter
* ( w_j^2 ) â†’ Squared model coefficients

**Effect of Ridge Regression:**

âœ” Shrinks coefficients toward zero
âœ” Reduces variance
âœ” Improves stability
âœ” Retains all features

---

### **3ï¸âƒ£ Lasso Regression (L1 Regularization)**

Lasso Regression applies an **L1 penalty**, enabling coefficient shrinkage and feature selection.

[
Loss = RSS + Î» \sum |w_j|
]

Where:

* **RSS** â†’ Residual Sum of Squares
* **Î» (lambda)** â†’ Regularization parameter
* ( |w_j| ) â†’ Absolute coefficient values

**Effect of Lasso Regression:**

âœ” Shrinks coefficients
âœ” Forces some coefficients = 0
âœ” Performs automatic feature selection
âœ” Improves interpretability

---

## ğŸ¯ Role of Regularization Parameter (Î»)

The parameter **Î» (lambda)** controls the strength of regularization:

* **Î» = 0** â†’ Equivalent to Linear Regression
* **Small Î»** â†’ Mild shrinkage
* **Large Î»** â†’ Strong shrinkage

Trade-off:

âœ” Higher Î» â†’ Less overfitting
âŒ Too large Î» â†’ Underfitting

---

âœ… Proper symbols
âœ… Academic formatting
âœ… Portfolio-grade

---

If you want next, I can add:

ğŸ“Š Coefficient shrinkage intuition
ğŸ“‰ Ridge vs Lasso geometry explanation
âœ¨ Math + visual combo section

Just tell me ğŸ˜„ğŸ”¥

---

## âš–ï¸ Ridge vs Lasso

| Aspect                | Ridge    | Lasso   |
| --------------------- | -------- | ------- |
| Regularization        | L2       | L1      |
| Feature Selection     | âŒ No     | âœ… Yes   |
| Coefficient Shrinkage | âœ… Yes    | âœ… Yes   |
| Model Complexity      | Moderate | Simpler |

---

## ğŸ› ï¸ Project Workflow

1ï¸âƒ£ Data Preprocessing
2ï¸âƒ£ Exploratory Data Analysis (EDA)
3ï¸âƒ£ Feature Encoding
4ï¸âƒ£ Feature Scaling (StandardScaler)
5ï¸âƒ£ Model Training
6ï¸âƒ£ Hyperparameter Tuning (GridSearchCV)
7ï¸âƒ£ Model Evaluation
8ï¸âƒ£ Final Model Selection
9ï¸âƒ£ Deployment (Streamlit)

---

## ğŸ“‰ Model Evaluation Metrics

Models were evaluated using:

* **RMSE (Root Mean Squared Error)**
* **RÂ² Score**

---

## ğŸ“Š Results Summary

| Model                | RMSE        | RÂ² Score     |
| -------------------- | ----------- | ------------ |
| Linear Regression    | 5199.74     | 0.9555       |
| Ridge Regression     | 5193.94     | 0.9556       |
| **Lasso Regression** | **5058.03** | **0.9579** âœ… |

---

## ğŸ† Final Model Selection

**Lasso Regression** was selected because:

âœ” Lowest RMSE (better accuracy)
âœ” Highest RÂ² Score
âœ” Automatic Feature Selection
âœ” Improved Interpretability

---

## ğŸ” Key Insights

âœ… Years of experience strongly influences salary
âœ… JobRole significantly impacts compensation
âœ… Performance rating positively affects salary
âœ… Lasso eliminated weak predictors

---

## ğŸ§° Technologies Used

* Python
* Pandas
* NumPy
* Scikit-learn
* Matplotlib / Seaborn
* Streamlit
* Joblib

---

## ğŸ“ Learning Outcomes

Through this project, I learned:

âœ… Data preprocessing techniques
âœ… Exploratory Data Analysis (EDA)
âœ… Regression modeling
âœ… Ridge vs Lasso regularization
âœ… Hyperparameter tuning
âœ… Model evaluation metrics
âœ… ML model deployment

---

## â­ Key Takeaway

> Regularization techniques like Ridge and Lasso improve model generalization, stability, and interpretability by controlling coefficient magnitudes and complexity.

Just tell me ğŸ˜„ğŸ”¥
