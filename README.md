---

# ðŸ’° Employee Salary Prediction using Lasso Regression

## ðŸ“Œ Project Overview
This project focuses on predicting **employee monthly salary** using Machine Learning techniques.
The goal is to build a robust regression model while addressing common challenges such as:

* Overfitting
* Multicollinearity
* Model complexity

To achieve this, we implemented and compared:

âœ… Linear Regression (Baseline)
âœ… Ridge Regression (L2 Regularization)
âœ… Lasso Regression (L1 Regularization)

The final model was deployed as an **interactive Streamlit web application**.

---

## ðŸŽ¯ Problem Statement

Accurate salary prediction is valuable for:

* Compensation planning
* Budget forecasting
* HR analytics
* Fair pay analysis

This project uses employee attributes like experience, education, and job role to predict salary.

---

## ðŸ“Š Dataset Description

The dataset represents structured HR analytics data containing employee-related features:

| Feature           | Description           |
| ----------------- | --------------------- |
| Age               | Employee age          |
| Gender            | Male / Female         |
| Department        | Functional department |
| JobRole           | Employee designation  |
| EducationLevel    | Qualification         |
| YearsExperience   | Total work experience |
| PerformanceRating | Performance score     |
| WorkHoursPerWeek  | Weekly work hours     |
| MonthlySalary     | **Target Variable**   |

---

## ðŸ§  Machine Learning Approach

### **1ï¸âƒ£ Linear Regression**

Baseline model for performance comparison.

---

### **2ï¸âƒ£ Ridge Regression (L2)**

Applies coefficient shrinkage to reduce model variance and handle multicollinearity.

[
Loss = RSS + \lambda \sum w^2
]

âœ” Stabilizes coefficients
âœ” Reduces overfitting

---

### **3ï¸âƒ£ Lasso Regression (L1)**

Performs coefficient shrinkage **and feature selection**.

[
Loss = RSS + \lambda \sum |w|
]

âœ” Eliminates weak predictors
âœ” Improves interpretability
âœ” Simplifies model

---

## âš–ï¸ Ridge vs Lasso

| Aspect                | Ridge    | Lasso   |
| --------------------- | -------- | ------- |
| Regularization        | L2       | L1      |
| Feature Selection     | âŒ No     | âœ… Yes   |
| Coefficient Shrinkage | âœ… Yes    | âœ… Yes   |
| Model Complexity      | Moderate | Simpler |

---

## ðŸ› ï¸ Project Workflow

1ï¸âƒ£ Data Preprocessing
2ï¸âƒ£ Exploratory Data Analysis (EDA)
3ï¸âƒ£ Feature Encoding
4ï¸âƒ£ Feature Scaling (StandardScaler)
5ï¸âƒ£ Model Training
6ï¸âƒ£ Hyperparameter Tuning (GridSearchCV)
7ï¸âƒ£ Model Evaluation
8ï¸âƒ£ Final Model Selection
9ï¸âƒ£ Deployment (Streamlit)

---

## ðŸ“‰ Model Evaluation Metrics

Models were evaluated using:

* **RMSE (Root Mean Squared Error)**
* **RÂ² Score**

---

## ðŸ“Š Results Summary

| Model                | RMSE        | RÂ² Score     |
| -------------------- | ----------- | ------------ |
| Linear Regression    | 5199.74     | 0.9555       |
| Ridge Regression     | 5193.94     | 0.9556       |
| **Lasso Regression** | **5058.03** | **0.9579** âœ… |

---

## ðŸ† Final Model Selection

**Lasso Regression** was selected because:

âœ” Lowest RMSE (better accuracy)
âœ” Highest RÂ² Score
âœ” Automatic Feature Selection
âœ” Improved Interpretability

---

## ðŸ” Key Insights

âœ… Years of experience strongly influences salary
âœ… JobRole significantly impacts compensation
âœ… Performance rating positively affects salary
âœ… Lasso eliminated weak predictors

---

## ðŸ§° Technologies Used

* Python
* Pandas
* NumPy
* Scikit-learn
* Matplotlib / Seaborn
* Streamlit
* Joblib

---

## ðŸŽ“ Learning Outcomes

Through this project, I learned:

âœ… Data preprocessing techniques
âœ… Exploratory Data Analysis (EDA)
âœ… Regression modeling
âœ… Ridge vs Lasso regularization
âœ… Hyperparameter tuning
âœ… Model evaluation metrics
âœ… ML model deployment

---

## â­ Key Takeaway

> Regularization techniques like Ridge and Lasso improve model generalization, stability, and interpretability by controlling coefficient magnitudes and complexity.
